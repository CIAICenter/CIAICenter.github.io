---
---
@inproceedings{
wang2023causal,
title={Causal Balancing for Domain Generalization},
author={Xinyi Wang and Michael Saxon and Jiachen Li and Hongyang Zhang and Kun Zhang and William Yang Wang},
booktitle={International Conference on Learning Representations},
year={2023},
abbr={ICLR},
url={https://openreview.net/forum?id=F91SROvVJ_6}
}


@inproceedings{
tang2023tier,
title={Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors},
author={Zeyu Tang and Yatong Chen and Yang Liu and Kun Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
abbr={ICLR},
url={https://openreview.net/forum?id=SZdfz5k7cd1}
}


@inproceedings{
fan2023calibration,
title={Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems},
author={Yewen Fan and Nian Si and Kun Zhang},
booktitle={International Conference on Learning Representations},
year={2023},
abbr={ICLR},
url={https://openreview.net/forum?id=wzlWiO_WY4}
}


@inproceedings{
li2023gain,
title={{GAIN}: On the Generalization of Instructional Action Understanding},
author={Junlong Li and Guangyi Chen and Yansong Tang and Jinan Bao and Kun Zhang and Jie Zhou and Jiwen Lu},
booktitle={International Conference on Learning Representations},
year={2023},
abbr={ICLR},
url={https://openreview.net/forum?id=RlPmWBiyp6w}
}

@inproceedings{
zheng2023scalable,
title={Scalable Estimation of Nonparametric Markov Networks with Mixed-Type Data},
author={Yujia Zheng and Ignavier Ng and Yewen Fan and Kun Zhang},
booktitle={International Conference on Learning Representations},
year={2023},
abbr={ICLR},
url={https://openreview.net/forum?id=qBvBycTqVJ}
}




@inproceedings{
feng2022factored,
title={Factored Adaptation for Non-Stationary Reinforcement Learning},
author={Fan Feng and Biwei Huang and Kun Zhang and Sara Magliacane},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
abbr={NeurIPS},
url={https://openreview.net/forum?id=VQ9fogN1q6e}
}





@InProceedings{pmlr-v162-xie22a,
  title = 	 {Identification of Linear Non-{G}aussian Latent Hierarchical Structure},
  author =       {Xie, Feng and Huang, Biwei and Chen, Zhengming and He, Yangbo and Geng, Zhi and Zhang, Kun},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {24370--24387},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  abbrev = 	 {ICML},
  pdf = 	 {https://proceedings.mlr.press/v162/xie22a/xie22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/xie22a.html},
  abstract = 	 {Traditional causal discovery methods mainly focus on estimating causal relations among measured variables, but in many real-world problems, such as questionnaire-based psychometric studies, measured variables are generated by latent variables that are causally related. Accordingly, this paper investigates the problem of discovering the hidden causal variables and estimating the causal structure, including both the causal relations among latent variables and those between latent and measured variables. We relax the frequently-used measurement assumption and allow the children of latent variables to be latent as well, and hence deal with a specific type of latent hierarchical causal structure. In particular, we define a minimal latent hierarchical structure and show that for linear non-Gaussian models with the minimal latent hierarchical structure, the whole structure is identifiable from only the measured variables. Moreover, we develop a principled method to identify the structure by testing for Generalized Independent Noise (GIN) conditions in specific ways. Experimental results on both synthetic and real-world data show the effectiveness of the proposed approach.}
}



@inproceedings{
montagna2022scalable,
title={Scalable Causal Discovery with Score Matching},
author={Francesco Montagna and Nicoletta Noceti and Lorenzo Rosasco and Kun Zhang and Francesco Locatello},
booktitle={NeurIPS 2022 Workshop on Score-Based Methods},
year={2022},
abbr={NeurIPS},
url={https://openreview.net/forum?id=v56PHv_W2A}
}


@misc{chen2022prompt,
    title={Prompt Learning with Optimal Transport for Vision-Language Models},
    author={Guangyi Chen and Weiran Yao and Xiangchen Song and Xinyue Li and Yongming Rao and Kun Zhang},
    year={2022},
    abbr={arXiv},
    eprint={2210.01253},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}


@InProceedings{pmlr-v162-kong22a,
  title = 	 {Partial disentanglement for domain adaptation},
  author =       {Kong, Lingjing and Xie, Shaoan and Yao, Weiran and Zheng, Yujia and Chen, Guangyi and Stojanov, Petar and Akinwande, Victor and Zhang, Kun},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  abbr={ICML},
  pages = 	 {11455--11472},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/kong22a/kong22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/kong22a.html},
  abstract = 	 {Unsupervised domain adaptation is critical to many real-world applications where label information is unavailable in the target domain. In general, without further assumptions, the joint distribution of the features and the label is not identifiable in the target domain. To address this issue, we rely on a property of minimal changes of causal mechanisms across domains to minimize unnecessary influences of domain shift. To encode this property, we first formulate the data generating process using a latent variable model with two partitioned latent subspaces: invariant components whose distributions stay the same across domains, and sparse changing components that vary across domains. We further constrain the domain shift to have a restrictive influence on the changing components. Under mild conditions, we show that the latent variables are partially identifiable, from which it follows that the joint distribution of data and labels in the target domain is also identifiable. Given the theoretical insights, we propose a practical domain adaptation framework, called iMSDA. Extensive experimental results reveal that iMSDA outperforms state-of-the-art domain adaptation algorithms on benchmark datasets, demonstrating the effectiveness of our framework.}
}


@InProceedings{pmlr-v151-ng22b,
  title = 	 { On the Convergence of Continuous Constrained Optimization for Structure Learning },
  author =       {Ng, Ignavier and Lachapelle, Sebastien and Rosemary Ke, Nan and Lacoste-Julien, Simon and Zhang, Kun},
  abbr={PMLR},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {8176--8198},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/ng22b/ng22b.pdf},
  url = 	 {https://proceedings.mlr.press/v151/ng22b.html},
  abstract = 	 { Recently, structure learning of directed acyclic graphs (DAGs) has been formulated as a continuous optimization problem by leveraging an algebraic characterization of acyclicity. The constrained problem is solved using the augmented Lagrangian method (ALM) which is often preferred to the quadratic penalty method (QPM) by virtue of its standard convergence result that does not require the penalty coefficient to go to infinity, hence avoiding ill-conditioning. However, the convergence properties of these methods for structure learning, including whether they are guaranteed to return a DAG solution, remain unclear, which might limit their practical applications. In this work, we examine the convergence of ALM and QPM for structure learning in the linear, nonlinear, and confounded cases. We show that the standard convergence result of ALM does not hold in these settings, and demonstrate empirically that its behavior is akin to that of the QPM which is prone to ill-conditioning. We further establish the convergence guarantee of QPM to a DAG solution, under mild conditions. Lastly, we connect our theoretical results with existing approaches to help resolve the convergence issue, and verify our findings in light of an empirical comparison of them. }
}


@inproceedings{
guo2023federated,
abbr={ICML},
title={Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach},
author={Han Guo and Philip Greengard and Hongyi Wang and Andrew Gelman and Eric Xing and Yoon Kim},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=dZrQR7OR11}
}


@misc{bian2023does,
    title={Does compressing activations help model parallel training?},
    author={Song Bian and Dacheng Li and Hongyi Wang and Eric P. Xing and Shivaram Venkataraman},
    year={2023},
    abbr={arXiv},
    eprint={2301.02654},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{zhang2022impact,
    abbr={arXiv},
    title={The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning},
    author={Hanlin Zhang and Yi-Fan Zhang and Li Erran Li and Eric Xing},
    year={2022},
    eprint={2212.08686},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{doi:10.1089/cmb.2022.0280,
author = {Wang, Haohan and Lopez, Oscar and Xing, Eric P. and Wu, Wei},
abbr={JCB},
title = {Kernel Mixed Model for Transcriptome Association Study},
journal = {Journal of Computational Biology},
volume = {29},
number = {12},
pages = {1353-1356},
year = {2022},
}

@misc{luu2022expeditious,
    abbr={arXiv},
    title={Expeditious Saliency-guided Mix-up through Random Gradient Thresholding},
    author={Minh-Long Luu and Zeyi Huang and Eric P. Xing and Yong Jae Lee and Haohan Wang},
    year={2022},
    eprint={2212.04875},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}



@misc{zhuang2022optimizing,
    abbr={arXiv},
    title={On Optimizing the Communication of Model Parallelism},
    author={Yonghao Zhuang and Hexu Zhao and Lianmin Zheng and Zhuohan Li and Eric P. Xing and Qirong Ho and Joseph E. Gonzalez and Ion Stoica and Hao Zhang},
    year={2022},
    eprint={2211.05322},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@InProceedings{10.1007/978-3-031-20053-3_39,
author="Shen, Zhiqiang
and Xing, Eric",
abbr={ECCV},
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="A Fast Knowledge Distillation Framework for Visual Recognition",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="673--690",
abstract="While Knowledge Distillation (KD) has been recognized as a useful tool in many visual tasks, such as supervised classification and self-supervised representation learning, the main drawback of a vanilla KD framework is its mechanism that consumes the majority of the computational overhead on forwarding through the giant teacher networks, making the entire learning procedure inefficient and costly. The recently proposed solution ReLabel suggests creating a label map for the entire image. During training, it receives the cropped region-level label by RoI aligning on a pre-generated entire label map, which allows for efficient supervision generation without having to pass through the teachers repeatedly. However, as the pre-trained teacher employed in ReLabel is from the conventional multi-crop scheme, there are various mismatches between the global label-map and region-level labels in this technique, resulting in performance deterioration compared to the vanilla KD. In this study, we present a Fast Knowledge Distillation (FKD) framework that replicates the distillation training phase and generates soft labels using the multi-crop KD approach, meanwhile training faster than ReLabel since no post-processes such as RoI align and softmax operations are used. When conducting multi-crop in the same image for data loading, our FKD is even more efficient than the traditional image classification framework. On ImageNet-1K, we obtain 80.1{\%} Top-1 accuracy on ResNet-50, outperforming ReLabel by 1.2{\%} while being faster in training and more flexible to use. On the distillation-based self-supervised learning task, we also show that FKD has an efficiency advantage.",
isbn="978-3-031-20053-3"
}


@InProceedings{10.1007/978-3-031-20053-3_42,
author="Shen, Zhiqiang
and Liu, Zechun
and Xing, Eric",
abbr={ECCV},
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Sliced Recursive Transformer",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="727--744",
abstract="We present a neat yet effective recursive operation on vision transformers that can improve parameter utilization without involving additional parameters. This is achieved by sharing weights across depth of transformer networks. The proposed method can obtain a substantial gain ({\$}{\$}{\backslash}sim {\$}{\$}∼2{\%}) simply using na{\"i}ve recursive operation, requires no special or sophisticated knowledge for designing principles of networks, and introduces minimal computational overhead to the training procedure. To reduce the additional computation caused by recursive operation while maintaining the superior accuracy, we propose an approximating method through multiple sliced group self-attentions across recursive layers which can reduce the cost consumption by 10--30{\%} without sacrificing performance. We call our model Sliced Recursive Transformer (SReT), a novel and parameter-efficient vision transformer design that is compatible with a broad range of other designs for efficient ViT architectures. Our best model establishes significant improvement on ImageNet-1K over state-of-the-art methods while containing fewer parameters. The proposed weight sharing mechanism by sliced recursion structure allows us to build a transformer with more than 100 or even 1000 shared layers with ease while keeping a compact size (13--15 M), to avoid optimization difficulties when the model is too large. The flexible scalability has shown great potential for scaling up models and constructing extremely deep vision transformers. Code is available at https://github.com/szq0214/SReT.",
isbn="978-3-031-20053-3"
}



@ARTICLE{9847356,
  author={Zhang, Gongjie and Luo, Zhipeng and Cui, Kaiwen and Lu, Shijian and Xing, Eric P.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Meta-DETR: Image-Level Few-Shot Detection with Inter-Class Correlation Exploitation}, 
  year={2022},
  abbr={TPAMI},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/TPAMI.2022.3195735}}

@ARTICLE{9842356,
  abbr={TNNLS},
  author={Lin, Shuai and Liu, Chen and Zhou, Pan and Hu, Zi-Yuan and Wang, Shuojia and Zhao, Ruihui and Zheng, Yefeng and Lin, Liang and Xing, Eric and Liang, Xiaodan},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Prototypical Graph Contrastive Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/TNNLS.2022.3191086}}



@inproceedings {280874,
abbr={OSDI},
author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {559--578},
url = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
publisher = {USENIX Association},
month = jul,
}

@InProceedings{Chavan_2022_CVPR,
    abbr={CVPR},
    author    = {Chavan, Arnav and Shen, Zhiqiang and Liu, Zhuang and Liu, Zechun and Cheng, Kwang-Ting and Xing, Eric P.},
    title     = {Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {4931-4941}
}

@inproceedings{li2022damp,
  abbr={NeurIPS},
  title={DAMP: Automatically Finding Model Parallel Strategies with Heterogeneity Awareness},
  author={Li, Dacheng and Wang, Hongyi and Xing, Eric P and Zhang, Hao},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{sreenivasan2022rare,
  abbr={NeurIPS},
  title={Rare Gems: Finding Lottery Tickets at Initialization},
  author={Sreenivasan, Kartik and Sohn, Jy-yong Sohn and Yang, Liu and Grinde, Matthew and Nagle, Alliot and Wang, Hongyi and Xing, Eric P and Lee, Kangwook and Papailiopoulos, Dimitris},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{huang2022masked,
  abbr={NeurIPS},
  title={Masked Generative Adversarial Networks are Robust Generation Learners},
  author={Huang, Jiaxing and Cui, Kaiwen and Guan, Dayan and Xiao, Aoran and Zhan, Fangneng and Lu, Shijian and Liao, Shengcai and Xing, Eric P},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{wang2022uncovering,
  abbr={NeurIPS},
  title={Uncovering the Structural Fairness in Graph Contrastive Learning},
  author={Wang, Ruijia and Wang, Xiao and Shi, Chuan and Song, Le},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}


@inproceedings{zheng2022sica,
  abbr={NeurIPS},
  title={On the Identifiability of Nonlinear ICA: Sparsity and Beyond},
  author={Zheng, Yujia and Ng, Ignavier and Zhang, Kun},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{dai2022indenpendence,
  abbr={NeurIPS},
  title={Independence Testing-Based Approach to Causal Discovery under Measurement Error and Linear Non-Gaussian Models},
  author={Dai, Haoyue and Spirtes, Peter and Zhang, Kun},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{yao2022tdrl,
  abbr={NeurIPS},
  title={Temporal Disentangled Representation Learning},
  author={Yao, Weiran and Chen, Guangyi and Zhang, Kun},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{xie2022dcr,
  abbr={NeurIPS},
  title={Unsupervised Image-to-Image Translation with Density Changing Regularization},
  author={Xie, Shaoan and Ho, Qirong and Zhang, Kun},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{huang2022latent,
  abbr={NeurIPS},
  title={Latent Hierarchical Causal Structure Discovery with Rank Constraints},
  author={Huang, Biwei and Low, Charles and Xie, Feng and Glymour, Clark  and Zhang, Kun},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{feng2022factored,
  abbr={NeurIPS},
  title={Factored Adaptation for Non-Stationary Reinforcement Learning},
  author={Feng, Fan and Huang, Biwei and Zhang, Kun and Magliacane, Sara },
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}



@inproceedings{yang2022causal,
  abbr={NeurIPS},
  title={Causal Discovery in Linear Latent Variable Models Subject to Measurement Error},
  author={Yang, Yuqin and Ghassami, AmirEmad and Nafea, Mohamed S and Kiyavash,  Negar  and Zhang, Kun and Shpitser, Ilya},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{zuo2022counterfactual,
  abbr={NeurIPS},
  title={Counterfactual Fairness with Partially Known Causal Graph},
  author={Zuo, Aoqi and Wei, Susan and Liu, Tongliang and Han, Bo and Zhang, Kun and Gong, Mingming},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{gao2022dmissdag,
  abbr={NeurIPS},
  title={DMissDAG: Causal Discovery in the Presence of Missing Data with Continuous Additive Noise Models},
  author={Gao, Erdun and Ng, Ignavier and Gong, Mingming and Shen, Li and Huang, Wei and Liu, Tongliang and Zhang, Kun and Bondell, Howard},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{zhang2022truncated,
  abbr={NeurIPS},
  title={Truncated Matrix Power Iteration for Differentiable DAG Learning},
  author={Zhang, Zhen and Ng, Ignavier and Gong, Dong and Liu, Yuhang and Abbasnejad, Ehsan M and Gong, Mingming and Zhang, Kun and Shi, Javen Qinfeng},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}


@article{hu2021panoramic,
  abbr={arXiv},
  title={Panoramic Learning with A Standardized Machine Learning Formalism},
  author={Hu, Zhiting and Xing, Eric P},
  journal={arXiv preprint arXiv:2108.07783},
  year={2021}
}

@inproceedings{qiao2021pollux,
  abbr={OSDI},
  title={Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning},
  author={Qiao, Aurick and Choe, Sang Keun and Subramanya, Suhas Jayaram and Neiswanger, Willie and Ho, Qirong and Zhang, Hao and Ganger, Gregory R and Xing, Eric P},
  booktitle={15th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation (the Jay Lepreau Best Paper Award)},
  year={2021}
}

@inproceedings{al2020federated,
  abbr={ICLR},
  title={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},
  author={Al-Shedivat, Maruan and Gillenwater, Jennifer and Xing, Eric and Rostamizadeh, Afshin},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{zheng2018dags,
  abbr={NeurIPS},
  title={Dags with no tears: Continuous optimization for structure learning},
  author={Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep K and Xing, Eric P},
  booktitle={Conference on Neural Information Processing Systems},
  year={2017}
}

@article{al2020contextual,
  abbr={JMLR},
  title={Contextual Explanation Networks.},
  author={Al-Shedivat, Maruan and Dubey, Avinava and Xing, Eric P},
  journal={Journal of Machine Learning Research},
  year={2020}
}

@article{kandasamy2020tuning,
  abbr={JMLR},
  title={Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly.},
  author={Kandasamy, Kirthevasan and Vysyaraju, Karun Raju and Neiswanger, Willie and Paria, Biswajit and Collins, Christopher R and Schneider, Jeff and Poczos, Barnabas and Xing, Eric P},
  journal={Journal of Machine Learning Research},
  year={2020}
}

@inproceedings{uang2021action,
  abbr={ICML},
  title={Action-Sufficient State Representation Learning for Control with Structural Constraints},
  author={Huang, Biwei and Lu, Chaochao and Leqi, Liu and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Glymour, Clark and Sch{\"o}lkopf, Bernhard and Zhang, Kun},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@inproceedings{huang2021adarl,
  abbr={ICLR},
  title={AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning},
  author={Huang, Biwei and Feng, Fan and Lu, Chaochao and Magliacane, Sara and Zhang, Kun},
  booktitle={International Conference on Learning Representations (Spotlight)},
  year={2022}
}

@inproceedings{yao2021learning,
  abbr={ICLR},
  title={Learning Temporally Latent Causal Processes from General Temporal Data},
  author={Yao, Weiran and Sun, Yuewen and Ho, Alex and Sun, Changyin and Zhang, Kun},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{adams2021identification,
  abbr={NeurIPS},
  title={Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases},
  author={Adams, Jeffrey and Hansen, Niels and Zhang, Kun},
  booktitle={Conference on Neural Information Processing Systems},
  year={2021}
}

@inproceedings{xie2021unaligned,
  abbr={ICCV},
  title={Unaligned image-to-image translation by learning to reweight},
  author={Xie, Shaoan and Gong, Mingming and Xu, Yanwu and Zhang, Kun},
  booktitle={Proceedings of the International Conference on Computer Vision},
  year={2021}
}

@inproceedings{zhang2020domain,
  abbr={NeurIPS},
  title={Domain adaptation as a problem of inference on graphical models},
  author={Zhang, Kun and Gong, Mingming and Stojanov, Petar and Huang, Biwei and Liu, Qingsong and Glymour, Clark},
  booktitle={Conference on Neural Information Processing Systems},
  year={2020}
}

@inproceedings{xie2020generalized,
  abbr={NeurIPS},
  title={Generalized independent noise condition for estimating latent variable causal graphs},
  author={Xie, Feng and Cai, Ruichu and Huang, Biwei and Glymour, Clark and Hao, Zhifeng and Zhang, Kun},
  booktitle={Conference on Neural Information Processing Systems (Spotlight)},
  year={2020}
}

@article{huang2020causal,
  abbr={JMLR},
  title={Causal Discovery from Heterogeneous/Nonstationary Data.},
  author={Huang, Biwei and Zhang, Kun and Zhang, Jiji and Ramsey, Joseph D and Sanchez-Romero, Ruben and Glymour, Clark and Sch{\"o}lkopf, Bernhard},
  journal={Journal of Machine Learning Research},
  year={2020}
}
